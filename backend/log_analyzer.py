#!/usr/bin/env python
"""
æ—¥å¿—åˆ†æå·¥å…·
åˆ†æè‡ªåŠ¨ä¸Šä¼ ç¨‹åºçš„æ—¥å¿—è¾“å‡ºï¼Œè¯†åˆ«é—®é¢˜ã€æ€§èƒ½ç“¶é¢ˆå’Œå¤„ç†ç»Ÿè®¡
"""

import os
import re
import json
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Any, Optional, Tuple
import statistics

# ç§»é™¤å¯è§†åŒ–ä¾èµ–ï¼Œä½¿ç”¨æ–‡æœ¬æŠ¥å‘Š
HAS_VISUALIZATION = False

try:
    import matplotlib.pyplot as plt
    HAS_VISUALIZATION = True
except ImportError:
    pass


class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨"""
    
    def __init__(self, log_dir='logs'):
        """åˆå§‹åŒ–æ—¥å¿—åˆ†æå™¨"""
        self.log_dir = Path(log_dir)
        self.log_pattern = re.compile(
            r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}) - '
            r'(?P<logger>\S+) - '
            r'(?P<level>\w+) - '
            r'(?P<function>\w+):(?P<line>\d+) - '
            r'(?P<message>.*)'
        )
        
    def parse_log_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """è§£ææ—¥å¿—æ–‡ä»¶"""
        entries = []
        current_entry = None
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:  # è·³è¿‡ç©ºè¡Œ
                        continue
                    
                    # å°è¯•è§£æä¸ºæ–°çš„æ—¥å¿—æ¡ç›®
                    entry = self.parse_log_line(line)
                    if entry:
                        # å¦‚æœæœ‰å½“å‰æ¡ç›®ï¼Œå…ˆä¿å­˜å®ƒ
                        if current_entry:
                            current_entry['file'] = file_path.name
                            entries.append(current_entry)
                        
                        # å¼€å§‹æ–°çš„æ¡ç›®
                        current_entry = entry
                        current_entry['line_number'] = line_num
                    else:
                        # è¿™æ˜¯å¤šè¡Œæ¶ˆæ¯çš„å»¶ç»­
                        if current_entry:
                            current_entry['message'] += '\n' + line
                        else:
                            # è°ƒè¯•ï¼šæ‰“å°æ— æ³•è§£æçš„è¡Œ
                            print(f"æ— æ³•è§£æç¬¬{line_num}è¡Œ: {line[:100]}")
                
                # ä¿å­˜æœ€åä¸€ä¸ªæ¡ç›®
                if current_entry:
                    current_entry['file'] = file_path.name
                    entries.append(current_entry)
                    
        except Exception as e:
            print(f"è§£ææ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        
        return entries
    
    def parse_log_line(self, line: str) -> Optional[Dict[str, Any]]:
        """è§£æå•è¡Œæ—¥å¿—"""
        # åŒ¹é…æ—¥å¿—æ ¼å¼: 2025-09-30 01:26:11,112 - __main__ - INFO - æ¶ˆæ¯
        log_pattern = r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}) - (\S+) - (\w+) - (.+)'
        match = re.match(log_pattern, line.strip())
        
        if not match:
            return None
        
        timestamp_str, logger_name, level, message = match.groups()
        
        try:
            # è§£ææ—¶é—´æˆ³ï¼Œå°†é€—å·æ›¿æ¢ä¸ºç‚¹å·ä»¥åŒ¹é…Pythonçš„å¾®ç§’æ ¼å¼
            timestamp_str_fixed = timestamp_str.replace(',', '.')
            timestamp = datetime.strptime(timestamp_str_fixed, '%Y-%m-%d %H:%M:%S.%f')
        except ValueError:
            return None
        
        return {
            'timestamp': timestamp,
            'logger': logger_name,
            'level': level,
            'message': message,
            'raw_line': line.strip()
        }
    
    def analyze_processing_performance(self, entries):
        """åˆ†æå¤„ç†æ€§èƒ½"""
        print("\n" + "="*60)
        print("ğŸ“Š å¤„ç†æ€§èƒ½åˆ†æ")
        print("="*60)
        
        # ç»Ÿè®¡å„ä¸ªæ­¥éª¤çš„è€—æ—¶
        step_times = defaultdict(list)
        current_process = {}
        
        for entry in entries:
            message = entry['message']
            timestamp = entry['timestamp']
            
            if 'å¼€å§‹å¤„ç†æ–‡ä»¶:' in message:
                current_process['start'] = timestamp
            elif 'æ­¥éª¤1: æå–æ–‡æ¡£å†…å®¹' in message:
                current_process['extract_start'] = timestamp
            elif 'æˆåŠŸæå–æ–‡æœ¬å†…å®¹' in message:
                if 'extract_start' in current_process:
                    duration = (timestamp - current_process['extract_start']).total_seconds()
                    step_times['æ–‡æ¡£æå–'].append(duration)
            elif 'æ­¥éª¤2: AIè§£æèŒä½æè¿°' in message:
                current_process['ai_start'] = timestamp
            elif 'AIè§£æå®Œæˆ' in message:
                if 'ai_start' in current_process:
                    duration = (timestamp - current_process['ai_start']).total_seconds()
                    step_times['AIè§£æ'].append(duration)
            elif 'æ­¥éª¤3: ä¿å­˜åˆ°æ•°æ®åº“' in message:
                current_process['db_start'] = timestamp
            elif 'æˆåŠŸä¿å­˜èŒä½åˆ°æ•°æ®åº“' in message:
                if 'db_start' in current_process:
                    duration = (timestamp - current_process['db_start']).total_seconds()
                    step_times['æ•°æ®åº“ä¿å­˜'].append(duration)
            elif 'æ–‡ä»¶å¤„ç†å®Œæˆ' in message:
                if 'start' in current_process:
                    duration = (timestamp - current_process['start']).total_seconds()
                    step_times['æ€»å¤„ç†æ—¶é—´'].append(duration)
                current_process = {}
        
        # è¾“å‡ºæ€§èƒ½ç»Ÿè®¡
        for step, times in step_times.items():
            if times:
                avg_time = sum(times) / len(times)
                min_time = min(times)
                max_time = max(times)
                print(f"\n{step}:")
                print(f"  å¹³å‡è€—æ—¶: {avg_time:.2f}ç§’")
                print(f"  æœ€çŸ­è€—æ—¶: {min_time:.2f}ç§’")
                print(f"  æœ€é•¿è€—æ—¶: {max_time:.2f}ç§’")
                print(f"  å¤„ç†æ¬¡æ•°: {len(times)}")
    
    def analyze_error_patterns(self, entries):
        """åˆ†æé”™è¯¯æ¨¡å¼"""
        print("\n" + "="*60)
        print("ğŸš¨ é”™è¯¯æ¨¡å¼åˆ†æ")
        print("="*60)
        
        error_entries = [e for e in entries if e['level'] == 'ERROR']
        
        if not error_entries:
            print("æ²¡æœ‰å‘ç°é”™è¯¯æ—¥å¿—")
            return
        
        # é”™è¯¯ç±»å‹ç»Ÿè®¡
        error_types = {}
        for entry in error_entries:
            message = entry['message']
            # æå–é”™è¯¯ç±»å‹
            if ':' in message:
                error_type = message.split(':')[0].strip()
            else:
                error_type = "æœªçŸ¥é”™è¯¯"
            
            error_types[error_type] = error_types.get(error_type, 0) + 1
        
        print(f"é”™è¯¯ç±»å‹åˆ†å¸ƒ:")
        for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):
            print(f"  {error_type}: {count}")
        
        # æ˜¾ç¤ºæœ€è¿‘çš„é”™è¯¯
        print(f"\næœ€è¿‘çš„é”™è¯¯ (æœ€å¤šæ˜¾ç¤º5æ¡):")
        recent_errors = sorted(error_entries, key=lambda x: x['timestamp'], reverse=True)[:5]
        for i, entry in enumerate(recent_errors, 1):
            print(f"{i}. [{entry['timestamp']}] {entry['message']}")
        
        # é”™è¯¯æ—¶é—´åˆ†å¸ƒ
        error_hours = {}
        for entry in error_entries:
            hour = entry['timestamp'].hour
            error_hours[hour] = error_hours.get(hour, 0) + 1
        
        if error_hours:
            print(f"\né”™è¯¯æ—¶é—´åˆ†å¸ƒ:")
            for hour in sorted(error_hours.keys()):
                print(f"  {hour:02d}:00-{hour:02d}:59: {error_hours[hour]} ä¸ªé”™è¯¯")
    
    def analyze_ai_performance(self, entries):
        """åˆ†æAIæ€§èƒ½"""
        print("\n" + "="*60)
        print("ğŸ¤– AIè§£ææ€§èƒ½åˆ†æ")
        print("="*60)
        
        ai_success = 0
        ai_failure = 0
        ai_response_lengths = []
        
        for entry in entries:
            message = entry['message']
            
            if 'AIè§£æå®Œæˆ' in message:
                ai_success += 1
            elif 'AIè§£æå¤±è´¥' in message:
                ai_failure += 1
            elif 'AIåŸå§‹å“åº”:' in message:
                # æå–å“åº”é•¿åº¦
                response_text = message.split('AIåŸå§‹å“åº”:')[1].strip()
                ai_response_lengths.append(len(response_text))
        
        total_ai_calls = ai_success + ai_failure
        
        if total_ai_calls > 0:
            success_rate = (ai_success / total_ai_calls) * 100
            print(f"\nAIè§£ææˆåŠŸç‡: {success_rate:.1f}% ({ai_success}/{total_ai_calls})")
            print(f"AIè§£æå¤±è´¥æ¬¡æ•°: {ai_failure}")
            
            if ai_response_lengths:
                avg_response_length = sum(ai_response_lengths) / len(ai_response_lengths)
                print(f"å¹³å‡å“åº”é•¿åº¦: {avg_response_length:.0f}å­—ç¬¦")
        else:
            print("æ²¡æœ‰å‘ç°AIè§£æè®°å½•")
    
    def analyze_database_operations(self, entries):
        """åˆ†ææ•°æ®åº“æ“ä½œ"""
        print("\n" + "="*60)
        print("ğŸ’¾ æ•°æ®åº“æ“ä½œåˆ†æ")
        print("="*60)
        
        db_saves = 0
        created_jobs = []
        
        for entry in entries:
            message = entry['message']
            
            if 'æˆåŠŸä¿å­˜èŒä½åˆ°æ•°æ®åº“' in message:
                db_saves += 1
                # æå–èŒä½ID
                if 'ID:' in message:
                    job_id = message.split('ID:')[1].strip().split()[0]
                    created_jobs.append(job_id)
            elif 'èŒä½è¯¦æƒ…:' in message:
                # æå–èŒä½è¯¦æƒ…
                details = message.split('èŒä½è¯¦æƒ…:')[1].strip()
                print(f"  åˆ›å»ºèŒä½: {details}")
        
        print(f"\næˆåŠŸä¿å­˜åˆ°æ•°æ®åº“: {db_saves}æ¬¡")
        print(f"åˆ›å»ºçš„èŒä½ID: {', '.join(created_jobs) if created_jobs else 'æ— '}")
    
    def generate_summary_report(self, entries):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“‹ å¤„ç†æ±‡æ€»æŠ¥å‘Š")
        print("="*60)
        
        if not entries:
            print("æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ¡ç›®")
            return
        
        # æ—¶é—´èŒƒå›´
        start_time = min(e['timestamp'] for e in entries)
        end_time = max(e['timestamp'] for e in entries)
        duration = end_time - start_time
        
        # æ—¥å¿—çº§åˆ«ç»Ÿè®¡
        level_counts = Counter(e['level'] for e in entries)
        
        # å¤„ç†çš„æ–‡ä»¶æ•°é‡
        processed_files = len([e for e in entries if 'å¼€å§‹å¤„ç†æ–‡ä»¶:' in e['message']])
        completed_files = len([e for e in entries if 'æ–‡ä»¶å¤„ç†å®Œæˆ' in e['message']])
        failed_files = len([e for e in entries if 'æ–‡ä»¶å¤„ç†å¤±è´¥:' in e['message']])
        
        print(f"\næ—¶é—´èŒƒå›´: {start_time.strftime('%Y-%m-%d %H:%M:%S')} - {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ€»æŒç»­æ—¶é—´: {duration}")
        print(f"æ€»æ—¥å¿—æ¡ç›®: {len(entries)}")
        
        print(f"\næ—¥å¿—çº§åˆ«åˆ†å¸ƒ:")
        for level, count in level_counts.most_common():
            print(f"  {level}: {count}")
        
        print(f"\næ–‡ä»¶å¤„ç†ç»Ÿè®¡:")
        print(f"  å¼€å§‹å¤„ç†: {processed_files}")
        print(f"  æˆåŠŸå®Œæˆ: {completed_files}")
        print(f"  å¤„ç†å¤±è´¥: {failed_files}")
        
        if processed_files > 0:
            success_rate = (completed_files / processed_files) * 100
            print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
    
    def analyze_logs(self, date_filter=None):
        """åˆ†ææ—¥å¿—"""
        print("ğŸ” å¼€å§‹åˆ†ææ—¥å¿—...")
        
        # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
        log_files = []
        if date_filter:
            pattern = f"auto_upload_{date_filter}*.log"
        else:
            pattern = "auto_upload_*.log"
        
        for log_file in self.log_dir.glob(pattern):
            if not log_file.name.endswith('_errors.log'):  # æ’é™¤é”™è¯¯ä¸“ç”¨æ—¥å¿—
                log_files.append(log_file)
        
        if not log_files:
            print(f"æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ—¥å¿—æ–‡ä»¶: {pattern}")
            return
        
        print(f"æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
        
        # è§£ææ‰€æœ‰æ—¥å¿—æ–‡ä»¶
        all_entries = []
        for log_file in sorted(log_files):
            print(f"è§£ææ—¥å¿—æ–‡ä»¶: {log_file}")
            entries = self.parse_log_file(log_file)
            all_entries.extend(entries)
        
        if not all_entries:
            print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ—¥å¿—æ¡ç›®")
            return
        
        print(f"æ€»å…±è§£æäº† {len(all_entries)} æ¡æ—¥å¿—")
        
        # æ‰§è¡Œå„ç§åˆ†æ
        self.generate_summary_report(all_entries)
        self.analyze_processing_performance(all_entries)
        self.analyze_error_patterns(all_entries)
        self.analyze_ai_performance(all_entries)
        self.analyze_database_operations(all_entries)
        
        print("\n" + "="*60)
        print("âœ… æ—¥å¿—åˆ†æå®Œæˆ")
        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åˆ†æè‡ªåŠ¨ä¸Šä¼ ç¨‹åºæ—¥å¿—')
    parser.add_argument('--date', help='åˆ†ææŒ‡å®šæ—¥æœŸçš„æ—¥å¿— (æ ¼å¼: YYYYMMDD)')
    parser.add_argument('--log-dir', default='logs', help='æ—¥å¿—ç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    analyzer = LogAnalyzer(args.log_dir)
    analyzer.analyze_logs(args.date)


if __name__ == '__main__':
    main()